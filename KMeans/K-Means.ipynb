{"cells":[{"cell_type":"markdown","source":["# K-means Clustering"],"metadata":{}},{"cell_type":"markdown","source":["The objective of this notebook is to develop a generic code to run K-Means clustering on numerical csv datasets. The final goal is to find clusters and report the average value for each feature by cluster in order to analyze the differences between them.\n\nFirst, we will read the data:"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/PDA_Data.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["And will take a look:"],"metadata":{}},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["We will get rid of the ID column since it is just an index:"],"metadata":{}},{"cell_type":"code","source":["df = df.select(\"Innovator\",\"Use Message\",\"Use Cell\",\"Use PIM\",\"Inf Passive\",\"Inf Active\",\"Remote Access\",\"Share Inf\",\"Monitor\",\"Email\",\"Web\",\"Mmedia\",\"Ergonomic\",\"Monthly\",\"Price\",\"Age\",\"Education\",\"Income\",\"Construction\",\"Emergency\",\"Sales\")\n\ndf.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df.printSchema"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Now, we need to prepare our data to train the K-Means model. For this we need to transform each row into a DenseVector and then we have to standardize the values. To accomplish that we will define the function 'prepare_data' that takes as input a dataframe and will return a standardized DenseVector:"],"metadata":{}},{"cell_type":"code","source":["def prepare_data(df):\n  \n  from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n  from pyspark.mllib.util import MLUtils\n  from pyspark.mllib.regression import LabeledPoint\n  \n  #We use the LabeledPoint object to transform the dataframe to a DenseVector\n  data = df.rdd.map(lambda line: LabeledPoint(0,[line[0:]]))\n  #We take just the features\n  features = data.map(lambda x: x.features)\n  #We fit and transform using the StandardScaler function\n  scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n  features_scale = scaler.transform(features)\n  return features_scale"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Using the function to create get the Standardized DenseVector:"],"metadata":{}},{"cell_type":"code","source":["features_scaled = prepare_data(df)\nfeatures_scaled.collect()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["The next step is to train the K-Means model to get the clusters. For this we will create a function that takes as input the features in DenseVector format, the number of clusters that you want to find and a seed in order to be able to replicate the results. The output is the trained K-means model and the cluster of each observation found by the model:"],"metadata":{}},{"cell_type":"code","source":["def kmeans_model(features, n_clusters, seed):\n  \n  from pyspark.mllib.clustering import KMeans, KMeansModel\n  \n  km_model = KMeans.train(features, n_clusters, maxIterations=10, initializationMode=\"random\",seed=seed)\n  clusters = km_model.predict(features)\n  \n  #from math import sqrt\n\n  #def error(point):\n  #    center = km_model.centers[km_model.predict(point)]\n  #    return sqrt(sum([x**2 for x in (point - center)]))\n\n  #WSSSE = features.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n  #print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n  \n  return km_model, clusters"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["We call the function to train the K-means model using the Standardized DenseVector that we created before, setting 5 clusters to be found, and 1234 as random seed. Then we display the clusters found by the model:"],"metadata":{}},{"cell_type":"code","source":["km_model, clusters = kmeans_model(features_scaled, 5, 1234)\nclusters.collect()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["We create this function to evaluate the Within Set Sum of Squared Error (WSSSE). The lower WSSSE, the better. This is a useful measure if you want to try different number of clusters and see which number fits the data better:"],"metadata":{}},{"cell_type":"code","source":["def WE(kmeans_model,features):\n  \n  from math import sqrt\n\n  def error(point):\n      center = kmeans_model.centers[kmeans_model.predict(point)]\n      return sqrt(sum([x**2 for x in (point - center)]))\n\n  WSSSE = features.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n  #print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n  return WSSSE"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["WE(km_model,features_scaled)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Now, we will put the cluster and the original dataframe together:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\ndef df_with_clusters(df,k_means_clusters):\n  \n  from pyspark.sql import Row\n\n  #We crete a dataframe columns with the clusters\n  row = Row('Cluster')\n  cluster = k_means_clusters.map(row).toDF()\n  \n  from pyspark.sql.types import StructType\n  from pyspark.sql.types import DataType\n  \n  #We create the structure of a new dataframe that will include an index value to join with the clusters column\n  Structure = df.schema[0:len(df.columns)]\n  Structure = Structure.add(\"index\", IntegerType(),True)\n  \n  #Create the dataframe with index\n  df_index = df.rdd.zipWithIndex().map(lambda (row, columnindex): row + (columnindex,)).toDF(StructType(Structure))\n  \n  #We create the structure type of a new dataframe for the clusters and an index to join with the features dataframe\n  Structure_type = StructType([cluster.schema[0],StructField(\"index\", IntegerType(),True)])\n  \n  #We create the dataframe with clusters and index\n  cluster_index = cluster.rdd.zipWithIndex().map(lambda (row, columnindex): row + (columnindex,)).toDF(Structure_type)\n\n  #We join the original feautres with the clusters and drop the index\n  df_final = df_index.join(cluster_index, df_index.index==cluster_index.index)\n  df_final = df_final.drop('index')\n  \n  return df_final"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Now that we have our function to join dataframes, we join the original dataframe with the clusters obtained using the K-Means model:"],"metadata":{}},{"cell_type":"code","source":["df_final = df_with_clusters(df,clusters)\ndf_final.show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Finally, we calculate the average value of each feature for each cluster:"],"metadata":{}},{"cell_type":"code","source":["def avg_by_cluster(df):\n  \n  exprs = {x: \"mean\" for x in df.columns}\n  df_by_cluster = df_final.groupBy(\"Cluster\").agg(exprs)\n  \n  return df_by_cluster"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df_avg_by_cluster = avg_by_cluster(df_final)\ndf_avg_by_cluster.show(3)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Saving the results:"],"metadata":{}},{"cell_type":"code","source":["df_avg_by_cluster.write.option(\"header\", True).csv(\"/FileStore/tables/clustering_results.csv\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Now you can use the results to check differences among clusters. For example:"],"metadata":{}},{"cell_type":"code","source":["df_avg_by_cluster.select('Cluster','avg(Age)','avg(Price)','avg(Innovator)').show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["############################################################################################################################################################"],"metadata":{}},{"cell_type":"markdown","source":["Working Notebook: This part contains some of the code used in the functions displayed above."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nrow = Row('Cluster')\ncluster = clusters.map(row).toDF()\ncluster.show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df.rdd.collect()\ncluster.rdd.collect()\n\ndf.rdd.zip(cluster.rdd).take(5)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql.types import StructType\n\ndef zip_df(l, r):\n    return l.rdd.zip(r.rdd).map(lambda x: (x[0][0],x[0][1],x[1][0])).toDF(StructType([l.schema[0],l.schema[1],r.schema[0]]))\n\ndf_final = zip_df(df, cluster.select('Cluster'))\ndf_final.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["from pyspark.sql.types import StructType\nfrom pyspark.sql.types import DataType"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nStructure = df.schema[0:21]\nStructure = Structure.add(\"index\", IntegerType(),True)\nStructure                       "],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["df.rdd.zipWithIndex().map(lambda (row, columnindex): row + (columnindex,)).toDF(StructType(Structure)).show(5)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["df_index = df.rdd.zipWithIndex().map(lambda (row, columnindex): row + (columnindex,)).toDF(StructType(Structure))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["Structure_type = StructType([cluster.schema[0],StructField(\"index\", IntegerType(),True)])\nStructure_type"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["cluster.rdd.zipWithIndex().map(lambda (row, columnindex): row + (columnindex,)).toDF(Structure_type).show(5)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["cluster_index = cluster.rdd.zipWithIndex().map(lambda (row, columnindex): row + (columnindex,)).toDF(Structure_type)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndf_final = df_index.join(cluster_index, df_index.index==cluster_index.index)\ndf_final.show(3)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["df_final = df_final.select(\"Innovator\",\"Use Message\",\"Use Cell\",\"Use PIM\",\"Inf Passive\",\"Inf Active\",\"Remote Access\",\"Share Inf\",\"Monitor\",\"Email\",\"Web\",\"Mmedia\",\"Ergonomic\",\"Monthly\",\"Price\",\"Age\",\"Education\",\"Income\",\"Construction\",\"Emergency\",\"Sales\",\"Cluster\")\n\ndf_final.show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["exprs = {x: \"mean\" for x in df.columns}\n\ndf_by_cluster = df_final.groupBy(\"Cluster\").agg(exprs)\n\ndf_by_cluster.show(3)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["df_by_cluster.toPandas().to_csv('clustering_results.csv')"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"K-Means","notebookId":723054752066269},"nbformat":4,"nbformat_minor":0}
