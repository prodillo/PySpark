{"cells":[{"cell_type":"markdown","source":["# Movie Recommender in PySpark"],"metadata":{}},{"cell_type":"markdown","source":["The objective of this notebook is to replicate Nick Pentreath's post: http://mlnick.github.io/blog/2013/04/01/movie-recommendations-and-more-with-spark/ in PySpark.\n\nWe will use the same data source: https://grouplens.org/datasets/movielens/ but we will take a smaller data file available."],"metadata":{}},{"cell_type":"markdown","source":["Fisrt we read the data and select: userId, movieId and rating:"],"metadata":{}},{"cell_type":"code","source":["ratings = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/ratings.csv\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["ratings.printSchema"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["ratings = ratings.select('userId','movieId','rating')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We will need the number of ratings for each movie, so we group by 'movieId' and count:"],"metadata":{}},{"cell_type":"code","source":["numRatersPerMovie = ratings.groupby('movieId').count()\nnumRatersPerMovie.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Now we join the number of ratings by movieId:"],"metadata":{}},{"cell_type":"code","source":["ratingsWithSize = ratings.join(numRatersPerMovie, ['movieId'] ,how='left')\nratingsWithSize = ratingsWithSize.selectExpr(\"movieId as movie\", \"userId as user\", \"rating as rating\" ,\"count as numRaters\")\nratingsWithSize.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Then we replicate the datframe ratingsWithSize and rename the columns. We will use this new dataframe to join it with itself on the userId to get all the pairs of movies that a user has rated:"],"metadata":{}},{"cell_type":"code","source":["ratings2 = ratingsWithSize.selectExpr(\"movie as movie2\", \"user as user2\", \"rating as rating2\" ,\"numRaters as numRaters2\")\nratings2.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Now we can make the join on user:"],"metadata":{}},{"cell_type":"code","source":["ratingPairs=ratingsWithSize.join(ratings2,ratingsWithSize.user==ratings2.user2)\nratingPairs.show(200)\nratingPairs.count()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["The next step is to get rid of all the duplicate movie pairs. For that we use the filter transformation and keep just those rows where movie Id is less than movie Id 2:"],"metadata":{}},{"cell_type":"code","source":["ratingPairs = ratingPairs.filter((\"movie < movie2\"))\nratingPairs.show()\nratingPairs.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Now we will create new features:\n\n- dotProduct = rating 1 * rating 2\n- ratingSq = (rating 1)**2\n- rating2Sq = (rating 2)**2"],"metadata":{}},{"cell_type":"code","source":["ratingPairs = ratingPairs.withColumn('dotProduct', ratingPairs.rating * ratingPairs.rating2)\nratingPairs.show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["ratingPairs = ratingPairs.withColumn('ratingSq', ratingPairs.rating**2)\nratingPairs = ratingPairs.withColumn('rating2Sq', ratingPairs.rating2**2)\nratingPairs.show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["ratingPairs.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["size = ratingPairs.groupby(['movie','movie2']).count()\nsize.show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["ratingPairs = ratingPairs.alias('a').join(size.alias('b'), (ratingPairs.movie == size.movie) & (ratingPairs.movie2 == size.movie2)).select('a.movie','a.user','a.rating','a.numRaters','a.movie2','a.user2','a.rating2','a.numRaters2','a.dotProduct','a.ratingSq','a.rating2Sq','b.count')\nratingPairs.show()\nratingPairs.count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["At this stage, we have to aggregate by each movie pair (movie,movie2):"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["vectorCalcs = ratingPairs.groupby(['movie','movie2']).agg(F.sum('dotProduct'), F.sum('rating'), F.sum('rating2'), F.sum('ratingSq'), F.sum('rating2Sq'), F.max('numRaters'), F.max('numRaters2'),F.max('count'))\nvectorCalcs.show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["We define the correlation function:"],"metadata":{}},{"cell_type":"code","source":["from numpy import sqrt"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["def correlation(size,dotProduct,ratingSum,rating2Sum, ratingNormSq, rating2NormSq):\n  numerator = size * dotProduct - ratingSum * rating2Sum\n  denominator = sqrt(size * ratingNormSq - ratingSum * ratingSum) * sqrt(size * rating2NormSq - rating2Sum * rating2Sum)*1.0\n\n  return (numerator*1.0/denominator)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["PRIOR_COUNT = 10\nPRIOR_CORRELATION = 0\n\ndef regularizedCorrelation(size, dotProduct, ratingSum, rating2Sum, ratingNormSq, rating2NormSq, virtualCount, priorCorrelation):\n  unregularizedCorrelation = correlation(size, dotProduct, ratingSum, rating2Sum, ratingNormSq, rating2NormSq)\n  w = size*1.0 / (size + virtualCount)\n\n  return (w * unregularizedCorrelation + (1 - w) * priorCorrelation)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["We define de Cosine similarity function:"],"metadata":{}},{"cell_type":"code","source":["def cosineSimilarity(dotProduct, ratingNorm, rating2Norm):\n  return dotProduct / (sqrt(ratingNorm) * sqrt(rating2Norm))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["And we apply the function to our vectors:"],"metadata":{}},{"cell_type":"code","source":["similarity=vectorCalcs.rdd.map(lambda x: (x[0],x[1],regularizedCorrelation(x[9],x[2],x[3],x[4],x[5],x[6],PRIOR_COUNT, PRIOR_CORRELATION),cosineSimilarity(x[2],x[5],x[6])))\nsimilarity.take(5)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["We save results to start from here:"],"metadata":{}},{"cell_type":"code","source":["similarity.saveAsTextFile(\"/FileStore/tables/similarity3\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Start from here with saved results:"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext\n\nsimilarity = sc.textFile(\"/FileStore/tables/similarity3\")\nsimilarity.take(5)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["We transform our RDD to dataframe:"],"metadata":{}},{"cell_type":"code","source":["results = similarity.map(lambda x: (int(x.replace('(','').replace(')','').split(',')[0]),int(x.replace('(','').replace(')','').split(',')[1]),float(x.replace('(','').replace(')','').split(',')[2]),float(x.replace('(','').replace(')','').split(',')[3]))).toDF(['movie','movie2','Reg_correlation','Cos_similarity'])\nresults.show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Now we read the movies file to get the movie names:"],"metadata":{}},{"cell_type":"code","source":["movies = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/movies.csv\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["movies.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Get the movies names from movies:"],"metadata":{}},{"cell_type":"code","source":["results_with_name=results.join(movies.selectExpr(\"movieId as movie\", \"title as name\"), ['movie'] ,how='left')\nresults_with_name.show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["results_with_names=results_with_name.join(movies.selectExpr(\"movieId as movie2\", \"title as name2\"), ['movie2'],how='left')\nresults_with_names.show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Finally, we can check some results:"],"metadata":{}},{"cell_type":"markdown","source":["Check the most similar movies to Die Hard (We remove NaNs in Reg_correlation, they are associated to those movie pairs rated just by one person):"],"metadata":{}},{"cell_type":"code","source":["results_with_names.where(results_with_names.name=='Die Hard (1988)').sort('Reg_correlation', ascending=False).na.drop().show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Check the most similar movies to Star Wars Episode IV:"],"metadata":{}},{"cell_type":"code","source":["results_with_names.where(results_with_names.name=='Star Wars: Episode IV - A New Hope (1977)').sort('Reg_correlation', ascending=False).na.drop().show()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Top ten most dissimilar to Star Wars:"],"metadata":{}},{"cell_type":"code","source":["results_with_names.where(results_with_names.name=='Star Wars: Episode IV - A New Hope (1977)').sort('Reg_correlation', ascending=True).na.drop().show(10)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Example of a NaN Reg_correlation value:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["vectorCalcs.filter((col(\"movie\")==1036) & (col(\"movie2\")==1980)).show()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":57}],"metadata":{"name":"Movie Recommender v2","notebookId":3938441360684029},"nbformat":4,"nbformat_minor":0}
